{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlliMulchandani/4501-Project2-Maps/blob/main/CS_4501_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSEhnnaikcAc",
        "outputId": "dc6577db-0af3-4661-f6dd-daea85885e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Map of All Location History\n",
        "---"
      ],
      "metadata": {
        "id": "yvrv2dIVDfG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import folium\n",
        "import numpy as np\n",
        "from geopy.distance import great_circle\n",
        "\n",
        "# Load JSON file\n",
        "with open(\"location-history.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract relevant data\n",
        "records = []\n",
        "for entry in data:\n",
        "    if \"visit\" in entry and \"topCandidate\" in entry[\"visit\"]:\n",
        "        place = entry[\"visit\"][\"topCandidate\"]\n",
        "        if \"placeLocation\" in place and place[\"placeLocation\"].startswith(\"geo:\"):\n",
        "            geo_parts = place[\"placeLocation\"].split(\":\")[1].split(\",\")  # Extract lat, lng\n",
        "            latitude, longitude = map(float, geo_parts)\n",
        "\n",
        "            # Compute time spent (endTime - startTime)\n",
        "            start_time = pd.to_datetime(entry[\"startTime\"])\n",
        "            end_time = pd.to_datetime(entry[\"endTime\"])\n",
        "            duration = (end_time - start_time).total_seconds() / 3600  # Convert to hours\n",
        "\n",
        "            records.append({\"latitude\": latitude, \"longitude\": longitude, \"duration\": duration})\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Group locations within 50 meters\n",
        "grouped_records = []\n",
        "while not df.empty:\n",
        "    base = df.iloc[0]  # Take the first location as base\n",
        "    close_points = df[df.apply(lambda row: great_circle((base[\"latitude\"], base[\"longitude\"]), (row[\"latitude\"], row[\"longitude\"])).meters < 50, axis=1)]\n",
        "\n",
        "    # Only keep groups with more than one instance\n",
        "    if len(close_points) > 1:\n",
        "        # Compute mean location and total duration\n",
        "        mean_lat = close_points[\"latitude\"].mean()\n",
        "        mean_lng = close_points[\"longitude\"].mean()\n",
        "        total_duration = close_points[\"duration\"].sum()\n",
        "\n",
        "        grouped_records.append({\"latitude\": mean_lat, \"longitude\": mean_lng, \"duration\": total_duration})\n",
        "\n",
        "    # Drop grouped points from DataFrame\n",
        "    df = df.drop(close_points.index)\n",
        "\n",
        "# Convert grouped data back to DataFrame\n",
        "df_grouped = pd.DataFrame(grouped_records)\n",
        "\n",
        "# Normalize duration for better visualization\n",
        "min_size, max_size = 3, 20  # Marker size range\n",
        "df_grouped[\"scaled_size\"] = np.interp(df_grouped[\"duration\"], (df_grouped[\"duration\"].min(), df_grouped[\"duration\"].max()), (min_size, max_size))\n",
        "\n",
        "df_grouped[\"color_intensity\"] = np.interp(df_grouped[\"duration\"], (df_grouped[\"duration\"].min(), df_grouped[\"duration\"].max()), (0.3, 1.0))\n",
        "\n",
        "# Create a map centered on the US\n",
        "us_center = [37.0902, -95.7129]  # Geographic center of the US\n",
        "m = folium.Map(location=us_center, zoom_start=4)\n",
        "\n",
        "# Add scaled markers\n",
        "for _, row in df_grouped.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
        "        radius=row[\"scaled_size\"],\n",
        "        color=\"red\",\n",
        "        fill=True,\n",
        "        fill_color=\"red\",\n",
        "        fill_opacity=row[\"color_intensity\"]\n",
        "    ).add_to(m)\n",
        "\n",
        "# Save and open map\n",
        "m.save(\"location_history_map.html\")\n",
        "print(\"Interactive map saved as location_history_map.html. Open it in a browser to view.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV7iDNQKiAVe",
        "outputId": "e597cc21-b459-4910-cb05-2f7d39951907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interactive map saved as location_history_map.html. Open it in a browser to view.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Map of 20 Most Significant Locations\n",
        "---"
      ],
      "metadata": {
        "id": "kG8NV-pWDWeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import folium\n",
        "import numpy as np\n",
        "from geopy.distance import great_circle\n",
        "\n",
        "# Load JSON file\n",
        "with open(\"location-history.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract relevant data\n",
        "records = []\n",
        "for entry in data:\n",
        "    if \"visit\" in entry and \"topCandidate\" in entry[\"visit\"]:\n",
        "        place = entry[\"visit\"][\"topCandidate\"]\n",
        "        if \"placeLocation\" in place and place[\"placeLocation\"].startswith(\"geo:\"):\n",
        "            geo_parts = place[\"placeLocation\"].split(\":\")[1].split(\",\")  # Extract lat, lng\n",
        "            latitude, longitude = map(float, geo_parts)\n",
        "\n",
        "            # Compute time spent (endTime - startTime)\n",
        "            start_time = pd.to_datetime(entry[\"startTime\"])\n",
        "            end_time = pd.to_datetime(entry[\"endTime\"])\n",
        "            duration = (end_time - start_time).total_seconds() / 3600  # Convert to hours\n",
        "\n",
        "            records.append({\"latitude\": latitude, \"longitude\": longitude, \"duration\": duration})\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Group locations within 50 meters\n",
        "grouped_records = []\n",
        "while not df.empty:\n",
        "    base = df.iloc[0]  # Take the first location as base\n",
        "    close_points = df[df.apply(lambda row: great_circle((base[\"latitude\"], base[\"longitude\"]), (row[\"latitude\"], row[\"longitude\"])).meters < 50, axis=1)]\n",
        "\n",
        "    # Only keep groups with more than one instance\n",
        "    if len(close_points) > 1:\n",
        "        # Compute mean location and total duration\n",
        "        mean_lat = close_points[\"latitude\"].mean()\n",
        "        mean_lng = close_points[\"longitude\"].mean()\n",
        "        total_duration = close_points[\"duration\"].sum()\n",
        "\n",
        "        grouped_records.append({\"latitude\": mean_lat, \"longitude\": mean_lng, \"duration\": total_duration})\n",
        "\n",
        "    # Drop grouped points from DataFrame\n",
        "    df = df.drop(close_points.index)\n",
        "\n",
        "# Convert grouped data back to DataFrame\n",
        "df_grouped = pd.DataFrame(grouped_records)\n",
        "\n",
        "# Group by location and sum the durations\n",
        "location_groups = df_grouped.groupby([\"latitude\", \"longitude\"]) [\"duration\"].sum().reset_index()\n",
        "\n",
        "# Sort by duration (descending) to get the top 20 locations\n",
        "top_20_locations = location_groups.sort_values(by=\"duration\", ascending=False).head(20)\n",
        "\n",
        "# Print the top 20 locations and their total time spent\n",
        "print(\"Top 20 locations with the most time spent:\")\n",
        "for index, row in top_20_locations.iterrows():\n",
        "    print(f\"Location: ({row['latitude']}, {row['longitude']}), Time Spent: {row['duration']:.2f} hours\")\n",
        "\n",
        "# Optional: Visualize the top 20 locations on the map\n",
        "for _, row in top_20_locations.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
        "        radius=7,\n",
        "        color=\"blue\",\n",
        "        fill=True,\n",
        "        fill_color=\"blue\",\n",
        "        fill_opacity=0.7\n",
        "    ).add_to(m)\n",
        "\n",
        "# Save and open updated map\n",
        "m.save(\"location_history_top_20_map.html\")\n",
        "print(\"Updated interactive map with top 20 locations saved as location_history_top_20_map.html. Open it in a browser to view.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE4XYVK8kPfp",
        "outputId": "ea05f451-3571-4e5c-e158-f17d0c0ffde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 locations with the most time spent:\n",
            "Location: ([REDACTED]), Time Spent: 602.53 hours\n",
            "Location: ([REDACTED]), Time Spent: 201.16 hours\n",
            "Location: ([REDACTED]), Time Spent: 67.50 hours\n",
            "Location: ([REDACTED]), Time Spent: 30.48 hours\n",
            "Location: ([REDACTED]), Time Spent: 27.06 hours\n",
            "Location: ([REDACTED]), Time Spent: 24.87 hours\n",
            "Location: ([REDACTED]), Time Spent: 19.03 hours\n",
            "...",
            "Updated interactive map with top 20 locations saved as location_history_top_20_map.html. Open it in a browser to view.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors\n",
        "---"
      ],
      "metadata": {
        "id": "4Y9li9wiDQ9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "# Charlottesville bounding coordinates -- I checked, this is a good boundary\n",
        "CHARLOTTESVILLE_BOUNDS = {\n",
        "    'min_lat': 37.95,  # Southern boundary\n",
        "    'max_lat': 38.15,  # Northern boundary\n",
        "    'min_lon': -78.60,  # Western boundary\n",
        "    'max_lon': -78.40   # Eastern boundary\n",
        "}\n",
        "\n",
        "# Load JSON file\n",
        "with open(\"location-history.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract sequential location data from entries that fall within Charlottesville bounds\n",
        "records = []\n",
        "for entry in data:\n",
        "    if \"visit\" in entry and \"topCandidate\" in entry[\"visit\"]:\n",
        "        place = entry[\"visit\"][\"topCandidate\"]\n",
        "        if \"placeLocation\" in place and place[\"placeLocation\"].startswith(\"geo:\"):\n",
        "            geo_parts = place[\"placeLocation\"].split(\":\")[1].split(\",\")  # Extract lat, lng\n",
        "            lat, lon = map(float, geo_parts)\n",
        "            # Apply the Charlottesville filter\n",
        "            if (CHARLOTTESVILLE_BOUNDS['min_lat'] <= lat <= CHARLOTTESVILLE_BOUNDS['max_lat'] and\n",
        "                CHARLOTTESVILLE_BOUNDS['min_lon'] <= lon <= CHARLOTTESVILLE_BOUNDS['max_lon']):\n",
        "                records.append((lat, lon))\n",
        "\n",
        "# Convert records to DataFrame\n",
        "df = pd.DataFrame(records, columns=[\"latitude\", \"longitude\"])\n",
        "\n",
        "# Prepare the data for training:\n",
        "# X: All points except the last (current location)\n",
        "# y: All points except the first (next location)\n",
        "X = df.iloc[:-1].values\n",
        "y = df.iloc[1:].values\n",
        "\n",
        "# Number of iterations (using different random_state values)\n",
        "num_iterations = 100\n",
        "\n",
        "# Lists to store metrics for each iteration\n",
        "cv_mse_list = []\n",
        "test_mse_list = []\n",
        "rmse_list = []\n",
        "distance_miles_list = []\n",
        "exact_match_accuracy_list = []\n",
        "THRESHOLD_METERS = 50  # threshold for an \"exact\" match\n",
        "\n",
        "for random_state in range(num_iterations):\n",
        "    # Split data into training and testing sets (80% train, 20% test)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "    # Train a K-Nearest Neighbors regressor\n",
        "    knn = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "    # Perform 5-fold cross-validation on the training set using MSE (negated)\n",
        "    cv_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "    cv_mse = -cv_scores.mean()\n",
        "    cv_mse_list.append(cv_mse)\n",
        "\n",
        "    # Fit the model on the full training set and predict on the test set\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    # Compute the Mean Squared Error (MSE) and RMSE on the test set\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    test_mse_list.append(mse)\n",
        "    rmse = np.sqrt(mse)\n",
        "    rmse_list.append(rmse)\n",
        "\n",
        "    # Estimate predicted distance in miles (using 1 degree ≈ 69 miles)\n",
        "    distance_miles = rmse * 69\n",
        "    distance_miles_list.append(distance_miles)\n",
        "\n",
        "    # Compute the \"Exact Match Accuracy\" (predicted location within 50 meters of true location)\n",
        "    # Convert threshold to degrees: latitude conversion is constant; longitude conversion adjusts with latitude.\n",
        "    lat_threshold = THRESHOLD_METERS / 111000  # approx conversion for latitude\n",
        "    # For each test sample, adjust longitude threshold based on its latitude\n",
        "    lon_thresholds = THRESHOLD_METERS / (111000 * np.cos(np.radians(y_test[:, 0])))\n",
        "\n",
        "    lat_diff = np.abs(y_pred[:, 0] - y_test[:, 0])\n",
        "    lon_diff = np.abs(y_pred[:, 1] - y_test[:, 1])\n",
        "\n",
        "    exact_matches = np.sum((lat_diff < lat_threshold) & (lon_diff < lon_thresholds))\n",
        "    exact_match_accuracy = exact_matches / len(y_test)\n",
        "    exact_match_accuracy_list.append(exact_match_accuracy)\n",
        "\n",
        "# Compute average metrics over all iterations\n",
        "avg_cv_mse = np.mean(cv_mse_list)\n",
        "avg_test_mse = np.mean(test_mse_list)\n",
        "avg_rmse = np.mean(rmse_list)\n",
        "avg_distance_miles = np.mean(distance_miles_list)\n",
        "avg_exact_match_accuracy = np.mean(exact_match_accuracy_list)\n",
        "\n",
        "# Output the averaged results\n",
        "print(f\"Average Cross-validation MSE over {num_iterations} iterations: {avg_cv_mse:.6f}\")\n",
        "print(f\"Average Test MSE over {num_iterations} iterations: {avg_test_mse:.6f}\")\n",
        "print(f\"Average RMSE over {num_iterations} iterations: {avg_rmse:.6f}\")\n",
        "print(f\"Average Predicted distance (in miles) over {num_iterations} iterations: {avg_distance_miles:.6f} miles\")\n",
        "print(f\"Average Exact Match Accuracy (within 50 meters) over {num_iterations} iterations: {avg_exact_match_accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw4-HOB8mnPx",
        "outputId": "47696be0-1880-408a-f694-0263c52e4ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Cross-validation MSE over 100 iterations: 0.000104\n",
            "Average Test MSE over 100 iterations: 0.000096\n",
            "Average RMSE over 100 iterations: 0.009589\n",
            "Average Predicted distance (in miles) over 100 iterations: 0.661655 miles\n",
            "Average Exact Match Accuracy (within 50 meters) over 100 iterations: 1.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression\n",
        "---"
      ],
      "metadata": {
        "id": "42SLipGXDDJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from geopy.distance import great_circle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "\n",
        "# Charlottesville bounding coordinates -- I checked, this is a good boundary\n",
        "CHARLOTTESVILLE_BOUNDS = {\n",
        "    'min_lat': 37.95,  # Southern boundary\n",
        "    'max_lat': 38.15,  # Northern boundary\n",
        "    'min_lon': -78.60,  # Western boundary\n",
        "    'max_lon': -78.40   # Eastern boundary\n",
        "}\n",
        "\n",
        "# Load JSON file\n",
        "with open(\"location-history.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract relevant data from JSON\n",
        "records = []\n",
        "for entry in data:\n",
        "    if \"visit\" in entry and \"topCandidate\" in entry[\"visit\"]:\n",
        "        place = entry[\"visit\"][\"topCandidate\"]\n",
        "        if \"placeLocation\" in place and place[\"placeLocation\"].startswith(\"geo:\"):\n",
        "            geo_parts = place[\"placeLocation\"].split(\":\")[1].split(\",\")  # Extract lat, lng\n",
        "            lat, lon = map(float, geo_parts)\n",
        "\n",
        "            # Charlottesville filter\n",
        "            if (CHARLOTTESVILLE_BOUNDS['min_lat'] <= lat <= CHARLOTTESVILLE_BOUNDS['max_lat'] and\n",
        "                CHARLOTTESVILLE_BOUNDS['min_lon'] <= lon <= CHARLOTTESVILLE_BOUNDS['max_lon']):\n",
        "\n",
        "                start_time = pd.to_datetime(entry[\"startTime\"])\n",
        "                end_time = pd.to_datetime(entry[\"endTime\"])\n",
        "                duration = (end_time - start_time).total_seconds() / 3600  # in hours\n",
        "                records.append({\"latitude\": lat, \"longitude\": lon, \"duration\": duration})\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Create pairs of consecutive locations for prediction\n",
        "X = []\n",
        "y = []\n",
        "for i in range(len(df) - 1):\n",
        "    current_location = df.iloc[i]\n",
        "    next_location = df.iloc[i + 1]\n",
        "\n",
        "    # Features: current location (latitude, longitude) and duration\n",
        "    X.append([current_location[\"latitude\"], current_location[\"longitude\"], current_location[\"duration\"]])\n",
        "    # Target: next location (latitude, longitude)\n",
        "    y.append([next_location[\"latitude\"], next_location[\"longitude\"]])\n",
        "\n",
        "# Convert lists to numpy arrays for easier handling\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Set the number of iterations (different random_state values)\n",
        "num_iterations = 100\n",
        "\n",
        "# Lists to store metrics for each iteration\n",
        "rmse_list = []\n",
        "mae_list = []\n",
        "mean_error_miles_list = []\n",
        "exact_match_accuracy_list = []\n",
        "\n",
        "# Define threshold for \"exact match\" in meters\n",
        "THRESHOLD_METERS = 50\n",
        "\n",
        "for random_state in range(num_iterations):\n",
        "    # Split data into training and testing sets using current random_state\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "    # Train a Linear Regression model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the next location on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Compute RMSE and MAE (units are degrees)\n",
        "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    # Calculate mean error in miles using great_circle distance\n",
        "    errors_in_miles = [\n",
        "        great_circle((y_true[0], y_true[1]), (y_hat[0], y_hat[1])).miles\n",
        "        for y_true, y_hat in zip(y_test, y_pred)\n",
        "    ]\n",
        "    mean_error = np.mean(errors_in_miles)\n",
        "\n",
        "    # Compute thresholds in degrees:\n",
        "    lat_threshold = THRESHOLD_METERS / 111000  # approx conversion for latitude\n",
        "    # For longitude, compute threshold for each test sample\n",
        "    lon_threshold = THRESHOLD_METERS / (111000 * np.cos(np.radians(y_test[:, 0])))\n",
        "\n",
        "    # Count predictions within threshold for both latitude and longitude\n",
        "    exact_matches = np.sum(\n",
        "        (np.abs(y_pred[:, 0] - y_test[:, 0]) < lat_threshold) &\n",
        "        (np.abs(y_pred[:, 1] - y_test[:, 1]) < lon_threshold)\n",
        "    )\n",
        "    exact_match_accuracy = exact_matches / len(y_test)\n",
        "\n",
        "    # Store metrics for this iteration\n",
        "    rmse_list.append(rmse)\n",
        "    mae_list.append(mae)\n",
        "    mean_error_miles_list.append(mean_error)\n",
        "    exact_match_accuracy_list.append(exact_match_accuracy)\n",
        "\n",
        "# Compute average metrics over all iterations\n",
        "avg_rmse = np.mean(rmse_list)\n",
        "avg_mae = np.mean(mae_list)\n",
        "avg_mean_error_miles = np.mean(mean_error_miles_list)\n",
        "avg_exact_match_accuracy = np.mean(exact_match_accuracy_list)\n",
        "\n",
        "print(f\"Average RMSE over {num_iterations} iterations: {avg_rmse:.6f} degrees\")\n",
        "print(f\"Average MAE over {num_iterations} iterations: {avg_mae:.6f} degrees\")\n",
        "print(f\"Average Mean Error in Miles over {num_iterations} iterations: {avg_mean_error_miles:.6f} miles\")\n",
        "print(f\"Average Exact Match Accuracy (within 50 meters) over {num_iterations} iterations: {avg_exact_match_accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjC-kyTVIF5K",
        "outputId": "c380fe5f-fa26-4c7c-b0c7-4e193aa23733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average RMSE over 100 iterations: 0.009213 degrees\n",
            "Average MAE over 100 iterations: 0.005551 degrees\n",
            "Average Mean Error in Miles over 100 iterations: 0.546282 miles\n",
            "Average Exact Match Accuracy (within 50 meters) over 100 iterations: 2.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Linear Regression Prediction\n",
        "---"
      ],
      "metadata": {
        "id": "QhB5JERaC16b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from geopy.distance import great_circle\n",
        "\n",
        "idx = np.random.randint(len(X_test))\n",
        "actual = y_test[idx]\n",
        "pred = y_pred[idx]\n",
        "\n",
        "m = folium.Map(location=actual, zoom_start=14)\n",
        "folium.Marker(actual, popup=\"Actual\").add_to(m)\n",
        "folium.Marker(pred, popup=f\"Predicted\\n{great_circle(actual, pred).miles:.1f}mi\").add_to(m)\n",
        "display(m)\n",
        "m.save(\"location_predictor_map.html\")"
      ],
      "metadata": {
        "id": "Hi9BhPsUHTj5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "outputId": "8885d90f-3a7e-44a0-f847-307c820cd216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<folium.folium.Map at 0x7cbbd318d950>"
            ],
            "text/html": [
              "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
              "&lt;html&gt;\n",
              "&lt;head&gt;\n",
              "    \n",
              "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
              "    \n",
              "        &lt;script&gt;\n",
              "            L_NO_TOUCH = false;\n",
              "            L_DISABLE_3D = false;\n",
              "        &lt;/script&gt;\n",
              "    \n",
              "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
              "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
              "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
              "    \n",
              "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
              "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
              "            &lt;style&gt;\n",
              "                #map_c95826ad60d47810b63d8cf24bda34c9 {\n",
              "                    position: relative;\n",
              "                    width: 100.0%;\n",
              "                    height: 100.0%;\n",
              "                    left: 0.0%;\n",
              "                    top: 0.0%;\n",
              "                }\n",
              "                .leaflet-container { font-size: 1rem; }\n",
              "            &lt;/style&gt;\n",
              "        \n",
              "&lt;/head&gt;\n",
              "&lt;body&gt;\n",
              "    \n",
              "    \n",
              "            &lt;div class=&quot;folium-map&quot; id=&quot;map_c95826ad60d47810b63d8cf24bda34c9&quot; &gt;&lt;/div&gt;\n",
              "        \n",
              "&lt;/body&gt;\n",
              "&lt;script&gt;\n",
              "    \n",
              "    \n",
              "            var map_c95826ad60d47810b63d8cf24bda34c9 = L.map(\n",
              "                &quot;map_c95826ad60d47810b63d8cf24bda34c9&quot;,\n",
              "                {\n",
              "                    center: [38.033553, -78.507977],\n",
              "                    crs: L.CRS.EPSG3857,\n",
              "                    ...{\n",
              "  &quot;zoom&quot;: 14,\n",
              "  &quot;zoomControl&quot;: true,\n",
              "  &quot;preferCanvas&quot;: false,\n",
              "}\n",
              "\n",
              "                }\n",
              "            );\n",
              "\n",
              "            \n",
              "\n",
              "        \n",
              "    \n",
              "            var tile_layer_a87f765fe8702b232b734f811601cae9 = L.tileLayer(\n",
              "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
              "                {\n",
              "  &quot;minZoom&quot;: 0,\n",
              "  &quot;maxZoom&quot;: 19,\n",
              "  &quot;maxNativeZoom&quot;: 19,\n",
              "  &quot;noWrap&quot;: false,\n",
              "  &quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;,\n",
              "  &quot;subdomains&quot;: &quot;abc&quot;,\n",
              "  &quot;detectRetina&quot;: false,\n",
              "  &quot;tms&quot;: false,\n",
              "  &quot;opacity&quot;: 1,\n",
              "}\n",
              "\n",
              "            );\n",
              "        \n",
              "    \n",
              "            tile_layer_a87f765fe8702b232b734f811601cae9.addTo(map_c95826ad60d47810b63d8cf24bda34c9);\n",
              "        \n",
              "    \n",
              "            var marker_97e894e09d9080311f01f0c6ae2e9991 = L.marker(\n",
              "                [38.033553, -78.507977],\n",
              "                {\n",
              "}\n",
              "            ).addTo(map_c95826ad60d47810b63d8cf24bda34c9);\n",
              "        \n",
              "    \n",
              "        var popup_4b32a418683796582766b7405f09a553 = L.popup({\n",
              "  &quot;maxWidth&quot;: &quot;100%&quot;,\n",
              "});\n",
              "\n",
              "        \n",
              "            \n",
              "                var html_ceebc0931e074fce4e171b82a315f39e = $(`&lt;div id=&quot;html_ceebc0931e074fce4e171b82a315f39e&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Actual&lt;/div&gt;`)[0];\n",
              "                popup_4b32a418683796582766b7405f09a553.setContent(html_ceebc0931e074fce4e171b82a315f39e);\n",
              "            \n",
              "        \n",
              "\n",
              "        marker_97e894e09d9080311f01f0c6ae2e9991.bindPopup(popup_4b32a418683796582766b7405f09a553)\n",
              "        ;\n",
              "\n",
              "        \n",
              "    \n",
              "    \n",
              "            var marker_52beaf946997601a70b0dd1917c45fea = L.marker(\n",
              "                [38.055788983029885, -78.49078559815626],\n",
              "                {\n",
              "}\n",
              "            ).addTo(map_c95826ad60d47810b63d8cf24bda34c9);\n",
              "        \n",
              "    \n",
              "        var popup_62197d29374eacef03af300e5ab93ad5 = L.popup({\n",
              "  &quot;maxWidth&quot;: &quot;100%&quot;,\n",
              "});\n",
              "\n",
              "        \n",
              "            \n",
              "                var html_301662be1fbcd2ca377220dbcd735c5f = $(`&lt;div id=&quot;html_301662be1fbcd2ca377220dbcd735c5f&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Predicted 1.8mi&lt;/div&gt;`)[0];\n",
              "                popup_62197d29374eacef03af300e5ab93ad5.setContent(html_301662be1fbcd2ca377220dbcd735c5f);\n",
              "            \n",
              "        \n",
              "\n",
              "        marker_52beaf946997601a70b0dd1917c45fea.bindPopup(popup_62197d29374eacef03af300e5ab93ad5)\n",
              "        ;\n",
              "\n",
              "        \n",
              "    \n",
              "&lt;/script&gt;\n",
              "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probabilistic Module\n",
        "---"
      ],
      "metadata": {
        "id": "1oB7XgT8CrRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# --- Data Extraction (as before) ---\n",
        "with open(\"location-history.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "CHARLOTTESVILLE_BOUNDS = {\n",
        "    'min_lat': 37.95,  # Southern boundary\n",
        "    'max_lat': 38.15,  # Northern boundary\n",
        "    'min_lon': -78.60,  # Western boundary\n",
        "    'max_lon': -78.40   # Eastern boundary\n",
        "}\n",
        "\n",
        "records = []\n",
        "for entry in data:\n",
        "    if \"visit\" in entry and \"topCandidate\" in entry[\"visit\"]:\n",
        "        place = entry[\"visit\"][\"topCandidate\"]\n",
        "        if \"placeLocation\" in place and place[\"placeLocation\"].startswith(\"geo:\"):\n",
        "            geo_parts = place[\"placeLocation\"].split(\":\")[1].split(\",\")\n",
        "            lat, lon = map(float, geo_parts)\n",
        "            if (CHARLOTTESVILLE_BOUNDS['min_lat'] <= lat <= CHARLOTTESVILLE_BOUNDS['max_lat'] and\n",
        "                CHARLOTTESVILLE_BOUNDS['min_lon'] <= lon <= CHARLOTTESVILLE_BOUNDS['max_lon']):\n",
        "                start_time = pd.to_datetime(entry[\"startTime\"])\n",
        "                end_time = pd.to_datetime(entry[\"endTime\"])\n",
        "                duration = (end_time - start_time).total_seconds() / 3600\n",
        "                records.append({\"latitude\": lat, \"longitude\": lon, \"duration\": duration})\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Create consecutive pairs:\n",
        "# X: current location features (latitude, longitude, duration)\n",
        "# y: next location (latitude, longitude)\n",
        "X, y = [], []\n",
        "for i in range(len(df) - 1):\n",
        "    current = df.iloc[i]\n",
        "    nxt = df.iloc[i + 1]\n",
        "    X.append([current[\"latitude\"], current[\"longitude\"], current[\"duration\"]])\n",
        "    y.append([nxt[\"latitude\"], nxt[\"longitude\"]])\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# --- Probabilistic Transition Model with Iterative Random Splits ---\n",
        "\n",
        "num_iterations = 100\n",
        "exact_match_accuracy_list = []\n",
        "THRESHOLD_METERS = 50\n",
        "\n",
        "for random_state in range(num_iterations):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "\n",
        "    # Build transition counts using X_train as keys and corresponding y_train as the next location.\n",
        "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
        "    for i in range(len(X_train) - 1):\n",
        "        current_loc = tuple(X_train[i])  # key is a tuple of (latitude, longitude, duration)\n",
        "        next_loc = tuple(y_train[i])      # target is a 2-tuple (latitude, longitude)\n",
        "        transition_counts[current_loc][next_loc] += 1\n",
        "\n",
        "    # Convert counts to probabilities.\n",
        "    transition_probs = {\n",
        "        loc: {next_loc: count / sum(next_steps.values())\n",
        "              for next_loc, count in next_steps.items()}\n",
        "        for loc, next_steps in transition_counts.items()\n",
        "    }\n",
        "\n",
        "    def predict_next_location(test_point):\n",
        "        test_tuple = tuple(test_point)\n",
        "        if test_tuple in transition_probs:\n",
        "            next_locs, probs = zip(*transition_probs[test_tuple].items())\n",
        "            return random.choices(next_locs, weights=probs)[0]\n",
        "        else:\n",
        "            # Find the nearest seen key (based on latitude and longitude only)\n",
        "            nearest_key = min(transition_probs.keys(),\n",
        "                              key=lambda loc: np.linalg.norm(np.array(loc[:2]) - np.array(test_point[:2])))\n",
        "            next_locs, probs = zip(*transition_probs[nearest_key].items())\n",
        "            return random.choices(next_locs, weights=probs)[0]\n",
        "\n",
        "    # Predict the next location for each test sample.\n",
        "    # The prediction now is always a 2-tuple (latitude, longitude).\n",
        "    y_pred_prob = np.array([predict_next_location(test_point) for test_point in X_test])\n",
        "\n",
        "    # Compute thresholds in degrees for matching:\n",
        "    lat_threshold = THRESHOLD_METERS / 111000  # latitude conversion\n",
        "    # For longitude, adjust for the latitude of each test sample.\n",
        "    lon_thresholds = THRESHOLD_METERS / (111000 * np.cos(np.radians(y_test[:, 0])))\n",
        "\n",
        "    lat_diff = np.abs(y_pred_prob[:, 0] - y_test[:, 0])\n",
        "    lon_diff = np.abs(y_pred_prob[:, 1] - y_test[:, 1])\n",
        "\n",
        "    exact_matches = np.sum((lat_diff < lat_threshold) & (lon_diff < lon_thresholds))\n",
        "    exact_match_accuracy = exact_matches / len(y_test)\n",
        "    exact_match_accuracy_list.append(exact_match_accuracy)\n",
        "\n",
        "avg_exact_match_accuracy = np.mean(exact_match_accuracy_list)\n",
        "print(f\"Average Exact Match Accuracy (within 50 meters, probabilistic model) over {num_iterations} iterations: {avg_exact_match_accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "QMBRRYnCw0Xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a161b5ec-fd0d-4461-c24e-01467f234ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Exact Match Accuracy (within 50 meters, probabilistic model) over 100 iterations: 19.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Places\n",
        "---"
      ],
      "metadata": {
        "id": "XYSQyYttBelk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_PLACES_API_KEY = userdata.get('GOOGLE_PLACES_API_KEY')\n",
        "\n",
        "# Google Places API URL\n",
        "GOOGLE_PLACES_API_URL = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
        "\n",
        "# Function to call Google Places API\n",
        "def get_google_place_info(lat, lon, radius=50):\n",
        "    params = {\n",
        "        \"location\": f\"{lat},{lon}\",\n",
        "        \"radius\": radius,\n",
        "        \"key\": GOOGLE_PLACES_API_KEY\n",
        "    }\n",
        "    response = requests.get(GOOGLE_PLACES_API_URL, params=params)\n",
        "    results = response.json().get(\"results\", [])\n",
        "\n",
        "    if results:\n",
        "        top_result = results[0]\n",
        "        name = top_result.get(\"name\", \"Unknown\")\n",
        "        address = top_result.get(\"vicinity\", \"No address available\")\n",
        "        categories = [type_ for type_ in top_result.get(\"types\", [])]\n",
        "        return {\"name\": name, \"address\": address, \"categories\": categories}\n",
        "    return None\n",
        "\n",
        "# Call the API for each of the top 10 locations\n",
        "print(\"\\nFetching place details from Google Places API...\\n\")\n",
        "for index, row in top_10_locations.iterrows():\n",
        "    place_info = get_google_place_info(row[\"latitude\"], row[\"longitude\"])\n",
        "    if place_info:\n",
        "        print(f\"Location: ({row['latitude']}, {row['longitude']}), Time Spent: {row['duration']:.2f} hours\")\n",
        "        print(f\"  - Name: {place_info['name']}\")\n",
        "        print(f\"  - Address: {place_info['address']}\")\n",
        "        print(f\"  - Categories: {', '.join(place_info['categories'])}\\n\")\n",
        "    else:\n",
        "        print(f\"Location: ({row['latitude']}, {row['longitude']}) - No details found.\\n\")\n"
      ],
      "metadata": {
        "id": "51x_pBzaBR0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Foursquare\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7-7WTlVPB43m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "FOURSQUARE_API_KEY = userdata.get('YOUR_FOURSQUARE_API_KEY')\n",
        "\n",
        "# Foursquare Places API URL\n",
        "FOURSQUARE_API_URL = \"https://api.foursquare.com/v3/places/search\"\n",
        "\n",
        "# Headers for API request\n",
        "HEADERS = {\n",
        "    \"Accept\": \"application/json\",\n",
        "    \"Authorization\": FOURSQUARE_API_KEY\n",
        "}\n",
        "\n",
        "# Function to call Foursquare API\n",
        "def get_foursquare_place_info(lat, lon, radius=50):\n",
        "    params = {\n",
        "        \"ll\": f\"{lat},{lon}\",\n",
        "        \"radius\": radius,\n",
        "        \"limit\": 1  # Get the closest place\n",
        "    }\n",
        "    response = requests.get(FOURSQUARE_API_URL, headers=HEADERS, params=params)\n",
        "    results = response.json().get(\"results\", [])\n",
        "\n",
        "    if results:\n",
        "        top_result = results[0]\n",
        "        name = top_result.get(\"name\", \"Unknown\")\n",
        "        address = top_result.get(\"location\", {}).get(\"formatted_address\", \"No address available\")\n",
        "        categories = [cat[\"name\"] for cat in top_result.get(\"categories\", [])]\n",
        "        return {\"name\": name, \"address\": address, \"categories\": categories}\n",
        "    return None\n",
        "\n",
        "# Call the API for each of the top 10 locations\n",
        "print(\"\\nFetching place details from Foursquare API...\\n\")\n",
        "for index, row in top_10_locations.iterrows():\n",
        "    place_info = get_foursquare_place_info(row[\"latitude\"], row[\"longitude\"])\n",
        "    if place_info:\n",
        "        print(f\"Location: ({row['latitude']}, {row['longitude']}), Time Spent: {row['duration']:.2f} hours\")\n",
        "        print(f\"  - Name: {place_info['name']}\")\n",
        "        print(f\"  - Address: {place_info['address']}\")\n",
        "        print(f\"  - Categories: {', '.join(place_info['categories'])}\\n\")\n",
        "    else:\n",
        "        print(f\"Location: ({row['latitude']}, {row['longitude']}) - No details found.\\n\")\n"
      ],
      "metadata": {
        "id": "8a_vOeUHB5SK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
